---
dropdown: API Documentation
title: FREME Datasets
pos: 1.3
layout: page
---

<script src="../js/jquery-ui-1.11.4/jquery-ui.min.js"></script>
<!--<script type="text/javascript" src="../js/htmlEncode.js"></script>-->
<!--<script type="text/javascript" src="../js/list-datasets.js"></script>-->
<link rel="stylesheet" href="../js/jquery-ui-1.11.4/jquery-ui.min.css">
This page lists the datasets that were converted in the FREME project. They can be used with FREME NER.
<div id="datasets">
<!--	<h3 id="dataseth0"></h3>
	<div id="datasetdiv0"></div> -->




	<!-- Start of FREME 0.4 Manually entered Datasets -->
	<h3><a target="_blank" href="http://datahub.io/dataset/orcid-dataset"> ORCID </a> </h3>
	<div>
		<h4> <strong>Description:</strong> </h4>
		<div>


			<p><a target="_blank" href="http://orcid.org/">ORCID</a> (Open Researcher and Contributor ID) is a nonproprietary alphanumeric code to uniquely identify scientific and other academic authors. This dataset contains RDF conversion of the ORCID dataset. The current conversion is based on the 2014 ORCID data dump, which contains around 1.3 million JSON files amounting to 41GB of data.
			</p>
			<p>The converted RDF version is 13GB large (uncompressed) and it is modelled with well known vocabularies such as Dublin Core, FOAF, schema.org, etc., and it is interlinked with GeoNames.
			</p>
			<p>Dump of the converted dataset can be downloaded from <a target="_blank" href="http://freme.aksw.org/datasets/orcid/">here</a>.
			</p>
			<p>Acknowledgments: The conversion of this corpus was supported by the <a target="_blank" href="http://www.freme-project.eu/">FREME H2020 project</a>.
			</p>

		</div>
		<h4> <strong>Author and Maintainer:</strong> </h4>
		<div> Milan Dojchinovski </div>
		<h4> <strong>URL:</strong> </h4>
		<div>  <a target="_blank" href="http://datahub.io/dataset/orcid-dataset">http://datahub.io/dataset/orcid-dataset</a> </div>
	</div>

	<h3><a target="_blank" href="http://datahub.io/dataset/statbel-corpus"> Statbel Corpus </a></h3>
	<div>
		<h4> <strong>Description:</strong></h4>
		<div>


			<p>This corpus contains RDF conversion of datasets from the "Statistics Belgium" (also known as Statbel) which aims at collecting, processing and disseminating relevant, reliable and commented data on Belgian society.
				<a target="_blank" href="http://statbel.fgov.be/en/statistics/figures/" target="_blank" rel="nofollow">http://statbel.fgov.be/en/statistics/figures/</a>
			</p>
			<p>Currently, the corpus contains three datasets:
			</p>
			<ul>
				<li><p><strong>Belgian house price index dataset</strong> (<a target="_blank" href="http://rv1460.1blu.de/datasets/statbel/house-price-index.ttl">dump</a>): measures the inflation on residential property market in Belgium. The data for conversion was obtained from [here].(<a target="_blank" href="http://statbel.fgov.be/en/statistics/figures/economy/construction_industry/house_price_index/" target="_blank" rel="nofollow">http://statbel.fgov.be/en/statistics/figures/economy/construction_industry/house_price_index/</a>)
				</p>

				</li>

				<li><p><strong>Employment, unemployment, labour market structure dataset</strong> (<a target="_blank" href="http://rv1460.1blu.de/datasets/statbel/employment-unemployment.ttl">dump</a>): data on employment, unemployment and the labour market from the labour force survey conducted among Belgian households. The data for conversion was obtained from <a target="_blank" href="http://statbel.fgov.be/en/statistics/figures/labour_market_living_conditions/employment/">here</a>.
				</p>

				</li>

				<li><p><strong>Unemployment and additional indicators dataset</strong> (<a target="_blank" href="http://rv1460.1blu.de/datasets/statbel/unemployment-indicators.ttl">dump</a>): contains unemployment related statistics about Belgium and its regions. The data for conversion was obtained from <a target="_blank" href="http://statbel.fgov.be/en/modules/publications/statistics/marche_du_travail_et_conditions_de_vie/unemployment_and_additional_indicators_2005-2010.jsp">here</a>.
				</p>

				</li>
			</ul>
			<p>The corpus is provided in RDF and it is modelled using the <a target="_blank" href="http://www.w3.org/TR/vocab-data-cube/">Data Cube vocabulary</a>.
			</p>
			<p>Acknowledgments: The conversion of this corpus was supported by the <a target="_blank" href="http://www.freme-project.eu/">FREME H2020 project</a>.
			</p>

		</div>
		<h4> <strong>Author and Maintainer:</strong> </h4>
		<div> Milan Dojchinovski </div>
		<h4> <strong>URL:</strong> </h4>
		<div>  <a target="_blank" href="http://datahub.io/dataset/statbel-corpus">http://datahub.io/dataset/statbel-corpus</a> </div>
	</div>

	<h3><a target="_blank" href="http://datahub.io/dataset/global-airports-in-rdf"> Global Airports in RDF </a></h3>
	<div>
		<h4><strong> Description: </strong></h4>
		<div>

			<p>This corpus contains RDF conversion of <a target="_blank" href="http://datahub.io/dataset/global-airports">Global airports dataset</a> which was retrieved from openflights.org. The dataset contains information about airport names, its location, codes, and other related info.
			</p>
			<p>The a dump from the dataset can be downloaded from <a target="_blank" href="http://rv1460.1blu.de/datasets/global-airports/global-airports.ttl">here</a>. The corpus is provided in RDF and it is interlinked with <a target="_blank" href="http://dbpedia.org/">DBpedia</a>.
			</p>
			<p>Acknowledgments: The conversion of this corpus was supported by the <a target="_blank" href="http://www.freme-project.eu/">FREME H2020 project</a>.
			</p>

		</div>
		<h4> <strong>Author and Maintainer:</strong> </h4>
		<div> Milan Dojchinovski </div>
		<h4> <strong>URL:</strong> </h4>
		<div>  <a target="_blank" href="http://datahub.io/dataset/global-airports-in-rdf">http://datahub.io/dataset/global-airports-in-rdf</a> </div>
	</div>
	<h3><a target="_blank" href="http://datahub.io/dataset/dbpedia-abstract-corpus"> DBpedia abstract corpus </a></h3>
	<div>
		<h4> <strong>Description:</strong> </h4>
		<div>
			<p>This corpus contains a conversion of Wikipedia abstracts in six languages (dutch, english, french, german, italian and spanish) into the I used the NLP Interchange Format (NIF). The corpus contains the abstract texts, as well as the position, surface form and linked article of all links in the text. As such, it contains entity mentions manually disambiguated to Wikipedia/DBpedia resources by native speakers, which predestines it for NER training and evaluation.
			</p>
			<p>Furthermore, the abstracts represent a special form of text that lends itself to be used for more sophisticated tasks, like open relation extraction. Their encyclopedic style, following Wikipedia guidelines on opening paragraphs adds further interesting properties. The first sentence puts the article in broader context. Most anaphers will refer to the original topic of the text, making them easier to resolve. Finally, should the same string occur in different meanings, Wikipedia guidelines suggest that the new meaning should again be linked for disambiguation. In short: The type of text is highly interesting.
			</p>
			<p>Acknowledgments: The conversion of this corpus was supported by the <a target="_blank" href="http://www.freme-project.eu/">FREME H2020 project</a>.
			</p>
		</div>
		<h4> <strong>Author and Maintainer:</strong> </h4>
		<div> Martin Br√ºmmer </div>
		<h4> <strong>URL:</strong> </h4>
		<div>  <a target="_blank" href="http://datahub.io/dataset/dbpedia-abstract-corpus">http://datahub.io/dataset/dbpedia-abstract-corpus</a> </div>
	</div>


</div>
<script javascript>
	$("#datasets").accordion();
</script>
